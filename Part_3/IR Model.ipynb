{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60b9675f",
   "metadata": {},
   "source": [
    "# IR Boolean Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc4c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4\n",
    "!pip install lxml\n",
    "!pip install hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f544f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import math\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import hazm as hzm\n",
    "import ast\n",
    "normalizer = hzm.Normalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ed7adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = 'dataset/Corpus/2007/'\n",
    "dataset = [f for f in listdir(mypath) if isfile(join(mypath,f))]\n",
    "dataset.remove('hamshahri.dtd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5410ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "for data in dataset:\n",
    "    raw_file = open(join(mypath,data),'r')\n",
    "    raw_file = raw_file.read()\n",
    "    bs = bs4.BeautifulSoup(raw_file,'xml')\n",
    "    \n",
    "\n",
    "    for el in bs.findAll('DOC'):\n",
    "        if el.TEXT:\n",
    "            body = el.TEXT.text\n",
    "            for im in el.TEXT.findAll('IMAGE'):\n",
    "                body = body.replace(im.text,'')\n",
    "            temp_dic = {\n",
    "                'title' : el.TITLE.text,\n",
    "                'body' : body,\n",
    "                'cat' : el.CAT.text\n",
    "            }\n",
    "            dic[el.DOCID.text] = temp_dic\n",
    "\n",
    "len(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8f0138",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocabulary = []\n",
    "\n",
    "for key in dic.keys():\n",
    "    tf_dict = {}\n",
    "    normalized_title = normalizer.normalize(dic[key]['title'])\n",
    "    tokenized_title = hzm.word_tokenize(normalized_title)\n",
    "    normalized_body = normalizer.normalize(dic[key]['body'])\n",
    "    tokenized_body = hzm.word_tokenize(normalized_body)\n",
    "    tokens = tokenized_title + tokenized_body\n",
    "\n",
    "    for token in tokens:\n",
    "        vocabulary.append(token)\n",
    "        if token in tf_dict.keys():\n",
    "            tf_dict[token] += 1\n",
    "        else:\n",
    "            tf_dict[token] =1\n",
    "    for word in tf_dict.keys():\n",
    "        tf_dict[word] /= len(tokens)\n",
    "    dic[key]['tf'] = tf_dict\n",
    "\n",
    "vocabulary = set(vocabulary)\n",
    "\n",
    "\n",
    "ID = []\n",
    "TF = []\n",
    "for key in dic.keys():\n",
    "    ID.append(key)\n",
    "    TF.append(dic[key]['tf'])\n",
    "\n",
    "DF_REF = {'ID':ID,'TF':TF }\n",
    "\n",
    "\n",
    "df_tf = pd.DataFrame(DF_REF)\n",
    "\n",
    "IDF = []\n",
    "for word in vocabulary:\n",
    "    DF = 0\n",
    "    for doc_id in dic.keys():\n",
    "            if word in dic[doc_id]['tf'].keys():\n",
    "                DF+=1\n",
    "    IDF.append(math.log((len(dic)/DF)))\n",
    "DF_REF = {'TERM':list(vocabulary), 'IDF':IDF}\n",
    "df_idf = pd.DataFrame(DF_REF)\n",
    "\n",
    "df_tf.to_csv('ID_TF.csv')\n",
    "df_idf.to_csv('TERM_IDF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09364e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf = pd.read_csv('TERM_IDF.csv')\n",
    "df_idf.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "df_idf.set_index('TERM',inplace=True)\n",
    "df_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5116ae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf = pd.read_csv('ID_TF.csv')\n",
    "df_tf.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "for i in range(len(df_tf)):\n",
    "    dic_tf = ast.literal_eval(df_tf.loc[i].TF)\n",
    "    for key in dic_tf.keys():\n",
    "        idf = df_idf.loc[key].IDF if key in df_idf.index else 0\n",
    "        dic_tf[key] = dic_tf[key] * idf\n",
    "    df_tf.loc[i].TF = dic_tf\n",
    "df_tf.rename(columns = {'TF':'WEIGHT'},inplace=True)\n",
    "df_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f670ae",
   "metadata": {},
   "source": [
    "## Read the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ac2af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_path = join('dataset','query-HAM2-FA-ENG','query-HAM2-FA.txt')\n",
    "queries_raw = open(query_path,'r').read()\n",
    "\n",
    "bs = bs4.BeautifulSoup(queries_raw,'xml')\n",
    "\n",
    "queries = {}\n",
    "for q in bs.findAll('QUERY'):\n",
    "    queries[int(q.ID.text)] = q.TITLE.text\n",
    "\n",
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1744e181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_query(q):\n",
    "    normalized_query = normalizer.normalize(q)\n",
    "    tokenized_query = hzm.word_tokenize(normalized_query)\n",
    "    tokenized_query\n",
    "    q_tf = {}\n",
    "    for token in tokenized_query:\n",
    "        if token in q_tf.keys():\n",
    "            q_tf[token] +=1\n",
    "        else:\n",
    "            q_tf[token] = 1\n",
    "    q_vector = {}\n",
    "    for key in q_tf.keys():\n",
    "        q_tf[key] /= len(tokenized_query)\n",
    "        idf = df_idf.loc[key].IDF if key in df_idf.index else 0\n",
    "        q_vector[key] = q_tf[key] * idf\n",
    "    return q_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e401fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(q_vector,doc_vector):\n",
    "    numerator = 0\n",
    "    for key in q_vector.keys():\n",
    "        if key in doc_vector.keys():\n",
    "            numerator += (doc_vector[key]*q_vector[key])\n",
    "    print(f'numerator = {numerator}')\n",
    "\n",
    "    if numerator == 0:\n",
    "        result = 0\n",
    "    else:\n",
    "        denominator = 0\n",
    "\n",
    "        q_vector_size = 0\n",
    "        for key in q_vector.keys():\n",
    "            q_vector_size += q_vector[key]**2\n",
    "        q_vector_size = math.sqrt(q_vector_size)\n",
    "\n",
    "        doc_vector_size = 0\n",
    "        for key in doc_vector.keys():\n",
    "            doc_vector_size += doc_vector[key]**2\n",
    "        doc_vector_size = math.sqrt(doc_vector_size)\n",
    "\n",
    "        denominator = doc_vector_size * q_vector_size\n",
    "\n",
    "        result = numerator/denominator\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47bd27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_result = {}\n",
    "for key in queries.keys():\n",
    "    q_vector = vectorize_query(queries[key])\n",
    "    print(f'Query is : {queries[key]}')\n",
    "    results = []\n",
    "    for i in range(len(df_tf)):\n",
    "        row = df_tf.loc[i]\n",
    "        doc_vector = row.WEIGHT\n",
    "        doc_id = row.ID\n",
    "        print(f'{doc_id} is beeing processed, {i}/{len(df_tf)}')\n",
    "        result = cosine_similarity(q_vector,doc_vector)\n",
    "        print(f'Similarity equals to {result}')\n",
    "        results.append((doc_id,result))\n",
    "    results = list(filter(lambda x:x[1] != 0,results))\n",
    "    results.sort(reverse=True, key = lambda x:x[1])\n",
    "    queries_result[queries[key]] = results\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa85bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('Queries_results','w')\n",
    "for key in queries_result.keys():\n",
    "    file.write(f'کوئری : {key}\\n')\n",
    "    file.write(f'۱۰ نتیجه اول : \\n')\n",
    "    results = queries_result[key][0:10]\n",
    "    for result in results:\n",
    "        file.write(f\"({result[0]},{dic[result[0]]['title']})\")\n",
    "    \n",
    "    file.write('\\n----------------------------------------------------------------------------------------\\n')\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
